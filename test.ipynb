{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import Graph\n",
    "from langchain.docstore.document import Document\n",
    "graph = Graph()\n",
    "\n",
    "inp = \"\"\"\\\n",
    "Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients. In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules. First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons. Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees. Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT.\\\n",
    "\"\"\"\n",
    "inp_1 = \"\"\"\\\n",
    "The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between Llama2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_input = [Document(page_content=inp), Document(page_content=inp_1)]\n",
    "graph.add(doc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 35.06it/s]\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'code': 'PGRST108', 'details': None, 'hint': \"Verify that 'eq' is included in the 'select' query parameter.\", 'message': \"'eq' is not an embedded resource in this request\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m node_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFederated Learning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m node_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLarge Language Models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_shortest_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/projects/cloth/graph.py:254\u001b[0m, in \u001b[0;36mGraph.find_shortest_path\u001b[0;34m(self, start_data, end_data)\u001b[0m\n\u001b[1;32m    248\u001b[0m current_node_id \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Retrieve all adjacent nodes\u001b[39;00m\n\u001b[1;32m    251\u001b[0m edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget(*), source(*)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mor_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource.eq.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_node_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget.eq.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_node_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m--> 254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00medges\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m edges:\n",
      "File \u001b[0;32m~/anaconda3/envs/local-ai/lib/python3.10/site-packages/postgrest/_sync/request_builder.py:78\u001b[0m, in \u001b[0;36mSyncQueryRequestBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m APIResponse[_ReturnT]\u001b[38;5;241m.\u001b[39mfrom_http_request_response(r)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(r\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(r\u001b[38;5;241m.\u001b[39mjson()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAPIError\u001b[0m: {'code': 'PGRST108', 'details': None, 'hint': \"Verify that 'eq' is included in the 'select' query parameter.\", 'message': \"'eq' is not an embedded resource in this request\"}"
     ]
    }
   ],
   "source": [
    "node_1 = \"Federated Learning\"\n",
    "node_2 = \"Large Language Models\"\n",
    "path = graph.find_shortest_path(node_1, node_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
